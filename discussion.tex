\section{Discussion}
From the experiments that we conducted on the 20 Newsgroups data set, it is clear that if the user chooses good representatives for each category, we can achieve high accuracy in the document categorization task.  This implies that the crucial step in our method is to present the user with suitable candidates for representatives.  We used Latent Dirichlet Allocation (LDA) for doing topic inferencing in our experiments and based on the latent topics discovered by LDA, we surface documents which have a high component of these latent topics.

Another alternative to LDA would be to use probabilistic Latent Semantic Analysis (pLSA) which treats topics as word distributions and uses probabilistic methods similar to LDA.  But with Dirichlet priors for the document-topic and topic-word distributions in LDA this prevents over-fitting, producing better results and is thus a better choice. Our goal in the future is to make topic inferencing better and, for that we will be experimenting with a modified version of LDA.  One idea is to use guided LDA as suggested in [1].  The seeds for guiding it will be the category name that the user decides to classify the batch of documents.  Another approach is to use Gaussian LDA as proposed in [2].  This approach is a good fit as it uses word embeddings too and we use the Glove word embeddings for the content classification by representing each document as a weighted average of the word embeddings.  A novel approach is to use ProdLDA as recommended in [3], which is a neural network version in which the distribution over individual words is a product of experts rather than the mixture model used in LDA.  Even clustering algorithms on the word embeddings representation of documents can be applied to do topic inferencing which we leave it for future work.

Although we did our experiments on single labeled datasets, we understand it can also apply it to multi-label datasets.  The idea is to use each label independently and the user will select the best representative for each label.  By treating each label independently of the other labels, we convert it into a single labeled problem.  The challenge is to recommend good candidate documents for being a representative for each label, and that's where standard LDA will fail as it will always suggest the same documents for being good candidates of being representatives to the user for all the labels.  That's where guiding the LDA with the seeds of category names can solve this problem.  An alternative approach opposed to the standard method of treating each label independently is to use classifier chains for multi-label classification as advised in [4].  This method can help us achieve a fair improvement in the accuracy if not significant.  The foremost thing again is the topic inferencing which could boost the accuracy even for the multi-label classification, and that's going to be our principal focus of research in the future.

[1] - http://legacydirs.umiacs.umd.edu/~jags/pdfs/GuidedLDA.pdf
[2] - https://rajarshd.github.io/papers/acl2015.pdf
[3] - https://arxiv.org/pdf/1703.01488.pdf
[4] - https://www.cs.waikato.ac.nz/ml/publications/2009/chains.pdf
