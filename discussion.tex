\section{Discussion and future work}
From the experiments that we conducted on the 20 Newsgroups and DBPedia datasets, it is clear that if the user chooses good representatives for each category, we can achieve high accuracy in the document categorization task. This implies that the crucial step in our method is to present the user with suitable candidates to be labeled as representatives.  We used Latent Dirichlet Allocation (LDA) for topic inference and found that while in many cases we got close to the maximum accuracy achieved through brute-force trials, in some case it fell far short or even failed to tease apart the topics at all.

An alternative to LDA would be to use probabilistic Latent Semantic Analysis (pLSA) which treats topics as word distributions and uses probabilistic methods similar to LDA. But with Dirichlet priors for the document-topic and topic-word distributions in LDA to prevent over-fitting, producing better results, this seems like a better choice. Our goal in the future is to improve the topic inference step and so we will look to other alternatives. One idea is to use guided LDA as suggested in [1].  The seeds for guiding it will be the category name that the user decides to classify the batch of documents.  Another approach is to use Gaussian LDA as proposed in [2].  This approach is a good fit as it uses word embeddings too and we use the Glove word embeddings for the content classification by representing each document as a weighted average of the word embeddings.  A novel approach is to use ProdLDA as recommended in [3], which is a neural network version in which the distribution over individual words is a product of experts rather than the mixture model used in LDA.  Even clustering algorithms on the word embeddings representation of documents can be applied to do topic inferencing which we leave it for future work.

Although we only tested our approach on single labeled datasets, we would like to be able to apply it to multi-labeled datasets. The idea is to use each label independently and the user will select the best representative for each label. By treating each label independently of the other labels, we convert it into a single labeled problem. The challenge is to recommend good candidate documents for being a representative for each label, and that's where standard LDA will fail as it will always suggest the same documents for being good candidates of being representatives to the user for all the labels.  That's where guiding the LDA with the seeds of category names can solve this problem.  An alternative approach opposed to the standard method of treating each label independently is to use classifier chains for multi-label classification as advised in [4].  This method can help us achieve a fair improvement in the accuracy.  The foremost thing again is the topic inferencing which could boost the accuracy even for the multi-label classification, and that's going to be our principal focus of research in the future.

[1] - http://legacydirs.umiacs.umd.edu/~jags/pdfs/GuidedLDA.pdf
[2] - https://rajarshd.github.io/papers/acl2015.pdf
[3] - https://arxiv.org/pdf/1703.01488.pdf
[4] - https://www.cs.waikato.ac.nz/ml/publications/2009/chains.pdf
