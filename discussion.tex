\section{Discussion}
Our testing on the 20 Newsgroups and DBPedia datasets showed that the general approach is sound, that the document embeddings using weighted averages of pre-trained word embeddings are useful representations, and that if good category representatives are chosen, high levels of accuracy can be achieved on the classification task. It also showed that LDA can help in choosing good representatives. The datasets we tested on were quite artificial: mostly just 2-category datasets chosen from larger datasets with more categories. However they did help identify certain characteristics that may make a real dataset suitable to use this approach on. The most important characteristic is that the categories must be sufficiently distinct and the words used in the documents should reflect those categories. The length of the documents chosen as representatives is also important - they should be roughly equal in length.

As to the question of working with more categories, why not test on the entire 20 Newsgroups or DBPedia dataset? Brute-force testing becomes infeasible with larger numbers of categories, and so it wasn't possible to get a maximum achievable accuracy for the entire dataset (the maximum achieved accuracy of 97\% on a particular 4-category subset of the DBPedia dataset was based on testing 390,625 of the 625 trillion possible combinations). In any case, it is unclear whether this approach will be appropriate for datasets with many categories --- say, more than four or five --- seeing as it requires the human in the loop to find good representatives for each one, a task which might prove daunting in such a case even if the category breakdown has been successfully approximated through LDA or similar. We feel that our text classification approach is best suited to 2- or 3-category classification tasks. An interesting use case for 2-category, i.e. binary, classification might be topic stance detection. Given user-generated content on a particular topic, e.g. posts on a web forum, it might be the case that different words are used depending on where the writer stands on that topic. An example of this would be some people using the term "family reunification" and others using "chain migration" to refer to the same thing. In this case our approach could be used to detect the stance reflected in each post.

\section{Future work}
The crucial step in our method is to present the user with suitable candidates to be labeled as representatives. We used Latent Dirichlet Allocation (LDA) for topic inference and found that while in many cases we got close to the maximum accuracy achieved through brute-force trials, in some cases it fell far short or even failed to tease apart the topics at all.

An alternative to LDA would be to use probabilistic Latent Semantic Analysis (pLSA) which treats topics as word distributions and uses probabilistic methods similar to LDA. But the use of Dirichlet priors in LDA for the document-topic and topic-word distributions in order to prevent over-fitting seems to make it a better choice. Our goal in the future is to improve the topic inference step, and so we will look to other alternatives. One idea is to use guided LDA as suggested in \citep{conf/eacl/JagarlamudiDU12}.  The seeds for guiding it will be the category names that the user specifies to classify the batch of documents.  Another approach is to use Gaussian LDA as proposed in \citep{conf/acl/DasZD15}. This approach is a good fit as it works with vectorized representations of words and documents. Yet another option would be to use ProdLDA as described in \citep{2017arXiv170301488S}, which is a neural network version in which the distribution over individual words is a product of experts rather than the mixture model used in LDA. Running clustering algorithms on the document representations is another approach we plan to try in order to solve the topic inference problem.

Although we only tested our approach on single labeled datasets, we would like to be able to apply it to multi-labeled datasets. One idea would be to use each label independently and do a binary classification of whether a document has that label or not. By treating each label independently of the other labels, we convert it into a single labeled problem. However, our current LDA approach to topic inference will not work in this case as it will always suggest the same ordering of documents regardless of which label the user is choosing representatives for. This is where guiding the LDA with the seeds of category names can prove immensely useful. An alternative approach, rather than treating each label independently, would be to use classifier chains for multi-label classification as suggested in \citep{Read2009}. This method may help us achieve a fair improvement in accuracy. The foremost issue again is the topic inference, which will be essential for accurate multi-label classification, and that's going to be our principal focus of research in the future.