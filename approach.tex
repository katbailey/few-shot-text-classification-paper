\section{Approach in Detail} \label{approach}
In this section, we provide the details of the text classification system.

\subsection{Document Embeddings}
 For any given collection of documents, which we refer to as a batch, each document needs to be converted to a fixed-length vector so that we can measure similarity between them. 

For our task, which is about classifying content, we found there to be little improvement in accuracy when running the PCA step, as suggested by \newcite{arora2017asimple} and \newcite{mu2017allbuttop}, over just using the weighted average of the word vectors. Morevover, PCA introduced a level of complexity that meant the embedding of a document was always batch-specific. For pragmatic reasons it was preferable for us to have document representations that were independent of the batch they came from. Hence our embedding method is simply:

\begin{algorithm}[!h]
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\Input{Word representations $\{v_w : w\in \mathcal{V}\}$, a set of documents $ \mathcal{D}$, parameter $\alpha$, and estimated probabilities  $\{p(w) : w\in \mathcal{V}\}$ of the words}
\Output{Document embeddings $\{v_d : d\in \mathcal{D}\}$.}
\For {all documents d in $\mathcal{D}$}{
	$ v_d \leftarrow \frac{1}{|d|} \sum_{w\in d} \frac{\alpha}{\alpha + p(w)}v_w$
	}

\caption{Weighted average algorithm for document representations.}
\label{algo:representation}
\end{algorithm}

\subsection{Classification}
The classification task involves manually labelling a small number of documents in each class and using these as representatives of the class. In the one-shot case, this means that our representation of a class is simply the single document vector that has been labeled for that class. Once we have a representative for each class, we can proceed to predict classes for the rest.

\begin{algorithm}[!h]
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\Input{Class representative vectors $\{v_c : c\in \mathcal{C}\}$, a set of document vectors $\{v_d : d\in \mathcal{D}\}$}
\Output{Predicted class for each document.}
\For {all documents d in $\mathcal{D}$}{
	\For {all classes c in $\mathcal{C}$} {
	$ sims_c \leftarrow cosine\_similarity(v_d, v_c)$
	}
	$\hat{c_d} \leftarrow argmax(sims)$
}
\caption{Predicting classes using cosine similarity with class representative vectors.}
\label{algo:classification}
\end{algorithm}

If we have more than one representative document per class, we simply take the average of those vectors as our category representative.

\subsection{Surfacing good representatives}
Choosing good class representatives is of vital importance to the accuracy of this approach. The human-in-the-loop will be making the final choice about which documents to use as representatives for each class, but we need to make it as easy as possible to choose the best possible representatives. Our approach is to run Latent Dirichlet Allocation (LDA) on the batch of documents, assign each document to the topic it has the highest probability of belonging to, and then rank the documents within each topic in descending order of their probability of belonging to the topic. The idea is to have an ordering of the documents within a batch such that the first page of documents seen in the UI is likely to have a mix of good representatives for each topic.
