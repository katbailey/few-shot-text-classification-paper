\section{Related work}
In \citep{arora2017asimple} the authors present an approach to representing sentences or entire documents as vectors. They first take the weighted average of the constituent (pre-trained) word vectors of the document. The weighting method serves to down-weight frequent words. They then run principal components analysis (PCA) on the batch, and the final embedding for each document is obtained by subtracting the projection of the set of sentence embeddings to their first principal component. The intuition behind this is that common methods, such as GloVe, for computing word vectors based on co-occurrence statistics lead to large components that contain no semantic information. A similar insight is presented in \citep{mu2017allbuttop}.

In \citep{snell2017prototypical}, the authors present the idea of \textit{Prototypical Networks} as a way of performing few-shot classification. In this approach, each class prototype is the mean vector of the vectorized support points belonging to its class and they are learned through gradient-descent-based training episodes on subsets of training examples. Classification then involves finding the nearest class prototype for each query vector. This is similar to how we perform classification but in our case we are actively seeking the best class prototypes by using a human-in-the-loop to choose them.