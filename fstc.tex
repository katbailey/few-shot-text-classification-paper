\documentclass{article} % For LaTeX2e
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc} % use 8-bit T1 fonts
\usepackage[letterpaper]{geometry}
\usepackage{latexsym}
\usepackage[linesnumbered,ruled]{algorithm2e}
\usepackage{multirow}
\usepackage{fstc,times}
\usepackage{url}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{caption}
\captionsetup[table]{
    labelsep=newline,
    justification=centering
    }
\usepackage{subcaption}
\makeatletter
\newcommand{\@BIBLABEL}{\@emptybiblabel}
\newcommand{\@emptybiblabel}[1]{}
\makeatother
% \usepackage[draft]{hyperref}
\usepackage[hidelinks]{hyperref}


\title{Few-Shot Text Classification with Pre-trained Word Embeddings and a Human in the Loop}


\author{Katherine Bailey \and Sunny Chopra \\
  Acquia \\
  \texttt{\{katherine.bailey,sunny.chopra\}@acquia.com}
}
\date{}


\begin{document}
\maketitle

\begin{abstract}
Most of the literature around text classification treats it as a supervised learning problem: given a corpus of labeled documents, train a classifier such that it can accurately predict the classes of unseen documents. In industry, however, it is not uncommon for a business to have entire corpora of documents where few or none have been classified, or where existing classifications have become meaningless. With web content, for example, poor taxonomy management can result in labels being applied indiscriminately, making filtering by these labels unhelpful. Our work aims to make it possible to classify an entire corpus of unlabeled documents using a human-in-the-loop approach, where the content owner manually classifies just one or two documents per category and the rest can be automatically classified. This "few-shot" learning approach requires rich representations of the documents such that those that have been manually labeled can be treated as prototypes, and automatic classification of the rest is a simple case of measuring the distance to prototypes. This approach uses pre-trained word embeddings, where documents are represented using a simple weighted average of constituent word embeddings. We have tested the accuracy of the approach on existing labeled datasets and provide the results here. We have also made code available for reproducing the results we got on the 20 Newsgroups dataset\footnote{\url{https://github.com/katbailey/few-shot-text-classification}}.
\end{abstract}

\section{Introduction}

\subsection*{Word Embeddings}

Word Embeddings are representations of words as low-dimensional vectors of real numbers that capture the semantic relationships between words. In Natural Language Processing (NLP), some method for converting words to numeric values is always necessary as computation only works with numbers, not raw text. \citep{mikolov2013distributed} introduced efficient techniques for learning distributed vector representations of words from huge corpora. This method is called word2vec, and since then alternative approaches have been put forward by others such as \citep{pennington2014glove}, known as GloVe, and \citep{bojanowski2016subword}, known as FastText. There is also doc2vec for learning representations of entire documents.

These approaches differ in the way the representations are generated.  Word2vec and doc2vec are "predictive" models, whereas GloVe is categorized as a "count-based" model.  Predictive models learn their vectors by predicting the target word given neighboring context words using a feed-forward neural network. The weights of this network are optimized by using stochastic gradient descent, and these weights are vector representation of the words in the vocabulary. In contrast, count-based models learn their vectors by finding a lower dimensional representation of each word by minimizing a reconstruction loss function on the co-occurrence count matrix given as an input.

\subsection*{Few-Shot Learning}
Few-shot learning is an approach to classification that works with only a few human labeled examples. It often goes hand-in-hand with transfer learning, a technique involving learning representations during one task that can then be applied to a different task, because it is the richness of the learned representations that makes it possible to learn from just a few examples. The use of pre-trained word embeddings is an example of transfer learning. One-Shot learning is a special case of Few-Shot Learning: as the name suggests, it means learning to classify objects when only one labeled example exists per class.

\subsection*{Human-in-the-Loop}
The term Human-in-the-Loop (HitL) refers to any Machine Learning technique that involves human input in the training process. It includes techniques such as active learning, where humans handle low confidence predictions, as well as crowd-sourced approaches to labeling data sets. In our case, the humans perform the "few shots" in our few-shot learning system.

\section{Few-Shot Text Classification with a Human in the Loop}
Our approach involves a "classification engine" that the user (the content owner) interacts with. A batch of documents is fed to the engine and each document is converted into a 300-dimensional vector. A set of two or more categories is specified and then Latent Dirichlet Allocation is run on the batch using the number of categories provided as the number of topics. This serves to surface the most likely representative documents for each category. These are then presented to the user, who must choose some documents to manually classify for each category. The system then has a vector representing each category: if only one document was classified for a category then that document's vector is used as the category vector, otherwise the vectors of multiple documents are averaged together. Once this is done, the remaining documents are compared against each category representative using simple cosine similarity and each one is assigned the category whose vector it is closest to. A score is also assigned for each prediction. All of these steps are explained in section \ref{approach}.

\input{relatedwork}

\input{approach}

\input{experiments}

\input{discussion}

% \clearpage
\bibliography{fstc}
\bibliographystyle{iclr2017_conference}

\end{document}


